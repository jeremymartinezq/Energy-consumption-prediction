{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Consumption Prediction\n",
    "A comprehensive system for predicting and managing household energy consumption using machine learning and time series forecasting techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy Consumption Prediction\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from prophet import Prophet\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Create directories for saving models and results\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def fetch_smart_meter_data(start_date='2023-01-01', end_date='2023-12-31', interval='1h'):\n",
    "    \"\"\"\n",
    "    Simulate fetching smart meter data from an API or database.\n",
    "    In a real scenario, this would connect to an actual data source.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching smart meter data from {start_date} to {end_date}...\")\n",
    "    \n",
    "    # Create a date range with the specified interval\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=interval)\n",
    "    \n",
    "    # Generate synthetic energy consumption data (kWh)\n",
    "    # Base consumption with daily and weekly patterns\n",
    "    n_samples = len(date_range)\n",
    "    base_consumption = 0.5 + 0.3 * np.sin(np.linspace(0, 2*np.pi*365, n_samples))  # Yearly cycle\n",
    "    daily_pattern = 0.2 * np.sin(np.linspace(0, 2*np.pi*n_samples, n_samples))     # Daily cycle\n",
    "    weekly_pattern = 0.1 * np.sin(np.linspace(0, 2*np.pi*52, n_samples))           # Weekly cycle\n",
    "    random_variation = 0.1 * np.random.randn(n_samples)                            # Random noise\n",
    "    \n",
    "    # Combine patterns\n",
    "    consumption = base_consumption + daily_pattern + weekly_pattern + random_variation\n",
    "    \n",
    "    # Scale to realistic kWh values (between 0.2 and 2 kWh per hour)\n",
    "    consumption = 0.2 + 1.8 * (consumption - consumption.min()) / (consumption.max() - consumption.min())\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': date_range,\n",
    "        'energy_consumption': consumption\n",
    "    })\n",
    "    \n",
    "    # Add some missing values (5% of data)\n",
    "    mask = np.random.choice([True, False], size=len(df), p=[0.05, 0.95])\n",
    "    df.loc[mask, 'energy_consumption'] = np.nan\n",
    "    \n",
    "    # Add outliers (1% of data)\n",
    "    outlier_mask = np.random.choice([True, False], size=len(df), p=[0.01, 0.99])\n",
    "    df.loc[outlier_mask, 'energy_consumption'] = df.loc[outlier_mask, 'energy_consumption'] * np.random.uniform(3, 5, size=sum(outlier_mask))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fetch_weather_data(location='New York', start_date='2023-01-01', end_date='2023-12-31'):\n",
    "    \"\"\"\n",
    "    Simulate fetching weather data from an API.\n",
    "    In a real scenario, this would connect to a weather API like OpenWeatherMap or DarkSky.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching weather data for {location} from {start_date} to {end_date}...\")\n",
    "    \n",
    "    # Create a date range with hourly interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='1h')\n",
    "    n_samples = len(date_range)\n",
    "    \n",
    "    # Generate synthetic weather data\n",
    "    # Temperature with seasonal pattern (°C)\n",
    "    season_factor = np.sin(np.linspace(0, 2*np.pi, 365*24))\n",
    "    seasonal_temp = 15 + 15 * np.repeat(season_factor, int(n_samples/(365*24)) + 1)[:n_samples]\n",
    "    \n",
    "    # Daily temperature variation\n",
    "    daily_factor = np.sin(np.linspace(0, 2*np.pi*n_samples, n_samples))\n",
    "    daily_temp = 5 * daily_factor\n",
    "    \n",
    "    # Random variations\n",
    "    random_temp = 3 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Combine patterns for temperature\n",
    "    temperature = seasonal_temp + daily_temp + random_temp\n",
    "    \n",
    "    # Humidity (%)\n",
    "    humidity = 60 + 20 * np.sin(np.linspace(0, 4*np.pi*365, n_samples)) + 10 * np.random.randn(n_samples)\n",
    "    humidity = np.clip(humidity, 0, 100)\n",
    "    \n",
    "    # Wind speed (m/s)\n",
    "    wind_speed = 5 + 3 * np.random.randn(n_samples)\n",
    "    wind_speed = np.clip(wind_speed, 0, 30)\n",
    "    \n",
    "    # Cloud cover (%)\n",
    "    cloud_cover = 50 + 30 * np.sin(np.linspace(0, 8*np.pi*365, n_samples)) + 20 * np.random.randn(n_samples)\n",
    "    cloud_cover = np.clip(cloud_cover, 0, 100)\n",
    "    \n",
    "    # Precipitation (mm)\n",
    "    precipitation = np.random.exponential(scale=0.5, size=n_samples)\n",
    "    precipitation_mask = np.random.choice([True, False], size=n_samples, p=[0.2, 0.8])\n",
    "    precipitation = precipitation * precipitation_mask\n",
    "    \n",
    "    # Create DataFrame\n",
    "    weather_df = pd.DataFrame({\n",
    "        'timestamp': date_range,\n",
    "        'temperature': temperature,\n",
    "        'humidity': humidity,\n",
    "        'wind_speed': wind_speed,\n",
    "        'cloud_cover': cloud_cover,\n",
    "        'precipitation': precipitation\n",
    "    })\n",
    "    \n",
    "    return weather_df\n",
    "\n",
    "# Fetch the data\n",
    "energy_df = fetch_smart_meter_data()\n",
    "weather_df = fetch_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display sample data\n",
    "print(\"\\nEnergy Consumption Data (First 5 rows):\")\n",
    "print(energy_df.head())\n",
    "\n",
    "print(\"\\nWeather Data (First 5 rows):\")\n",
    "print(weather_df.head())\n",
    "\n",
    "# Save raw data\n",
    "energy_df.to_csv('data/raw_energy_data.csv', index=False)\n",
    "weather_df.to_csv('data/raw_weather_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def preprocess_energy_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess energy consumption data:\n",
    "    - Handle missing values\n",
    "    - Remove outliers\n",
    "    - Extract datetime features\n",
    "    - Normalize consumption values\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing energy consumption data...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original data\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Handle missing values using forward fill and then backward fill\n",
    "    print(f\"Missing values before imputation: {processed_df['energy_consumption'].isna().sum()}\")\n",
    "    processed_df['energy_consumption'] = processed_df['energy_consumption'].interpolate(method='time').ffill().bfill()\n",
    "    print(f\"Missing values after imputation: {processed_df['energy_consumption'].isna().sum()}\")\n",
    "    \n",
    "    # Detect and handle outliers using IQR method\n",
    "    Q1 = processed_df['energy_consumption'].quantile(0.25)\n",
    "    Q3 = processed_df['energy_consumption'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_mask = (processed_df['energy_consumption'] < lower_bound) | (processed_df['energy_consumption'] > upper_bound)\n",
    "    print(f\"Number of outliers detected: {outliers_mask.sum()}\")\n",
    "    \n",
    "    # Cap outliers instead of removing them\n",
    "    processed_df.loc[processed_df['energy_consumption'] < lower_bound, 'energy_consumption'] = lower_bound\n",
    "    processed_df.loc[processed_df['energy_consumption'] > upper_bound, 'energy_consumption'] = upper_bound\n",
    "    \n",
    "    # Extract datetime features\n",
    "    processed_df['hour'] = processed_df['timestamp'].dt.hour\n",
    "    processed_df['day'] = processed_df['timestamp'].dt.day\n",
    "    processed_df['day_of_week'] = processed_df['timestamp'].dt.dayofweek\n",
    "    processed_df['month'] = processed_df['timestamp'].dt.month\n",
    "    processed_df['year'] = processed_df['timestamp'].dt.year\n",
    "    processed_df['is_weekend'] = processed_df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Add time-based flags\n",
    "    processed_df['is_morning'] = ((processed_df['hour'] >= 6) & (processed_df['hour'] < 12)).astype(int)\n",
    "    processed_df['is_afternoon'] = ((processed_df['hour'] >= 12) & (processed_df['hour'] < 18)).astype(int)\n",
    "    processed_df['is_evening'] = ((processed_df['hour'] >= 18) & (processed_df['hour'] < 22)).astype(int)\n",
    "    processed_df['is_night'] = ((processed_df['hour'] >= 22) | (processed_df['hour'] < 6)).astype(int)\n",
    "    \n",
    "    # Add lagged features\n",
    "    processed_df['consumption_lag_1h'] = processed_df['energy_consumption'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    processed_df['consumption_lag_24h'] = processed_df['energy_consumption'].shift(24)\n",
    "    processed_df['consumption_lag_168h'] = processed_df['energy_consumption'].shift(168)  # 1 week\n",
    "    \n",
    "    # Add rolling mean features\n",
    "    processed_df['rolling_mean_6h'] = processed_df['energy_consumption'].rolling(window=6).mean()\n",
    "    processed_df['rolling_mean_24h'] = processed_df['energy_consumption'].rolling(window=24).mean()\n",
    "    processed_df['rolling_mean_168h'] = processed_df['energy_consumption'].rolling(window=168).mean()\n",
    "    \n",
    "    # Add rolling standard deviation\n",
    "    processed_df['rolling_std_24h'] = processed_df['energy_consumption'].rolling(window=24).std()\n",
    "    \n",
    "    # Calculate daily and weekly seasonality\n",
    "    processed_df['daily_mean'] = processed_df.groupby(['hour'])['energy_consumption'].transform('mean')\n",
    "    processed_df['weekly_mean'] = processed_df.groupby(['day_of_week', 'hour'])['energy_consumption'].transform('mean')\n",
    "    \n",
    "    # Drop rows with NaN values (caused by lag and rolling features)\n",
    "    processed_df = processed_df.dropna()\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "def preprocess_weather_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess weather data:\n",
    "    - Handle missing values\n",
    "    - Extract additional features\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing weather data...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original data\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    processed_df = processed_df.interpolate(method='time').ffill().bfill()\n",
    "    \n",
    "    # Create additional weather features\n",
    "    processed_df['feels_like'] = processed_df['temperature'] - 0.4 * (processed_df['wind_speed'] / 10) * (33 - processed_df['temperature'])\n",
    "    processed_df['heat_index'] = processed_df['temperature'] + 0.05 * processed_df['humidity']\n",
    "    \n",
    "    # Create weather condition categories\n",
    "    processed_df['is_rainy'] = (processed_df['precipitation'] > 0.5).astype(int)\n",
    "    processed_df['is_windy'] = (processed_df['wind_speed'] > 10).astype(int)\n",
    "    processed_df['is_humid'] = (processed_df['humidity'] > 70).astype(int)\n",
    "    processed_df['is_cold'] = (processed_df['temperature'] < 10).astype(int)\n",
    "    processed_df['is_hot'] = (processed_df['temperature'] > 25).astype(int)\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# Preprocess the datasets\n",
    "processed_energy_df = preprocess_energy_data(energy_df)\n",
    "processed_weather_df = preprocess_weather_data(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def merge_data_and_engineer_features(energy_df, weather_df):\n",
    "    \"\"\"\n",
    "    Merge energy and weather data and create additional features.\n",
    "    \"\"\"\n",
    "    print(\"Merging datasets and engineering features...\")\n",
    "    \n",
    "    # Merge datasets on timestamp\n",
    "    merged_df = pd.merge(energy_df, weather_df, on='timestamp', how='inner')\n",
    "    \n",
    "    # Create interaction features\n",
    "    merged_df['temp_humid_interaction'] = merged_df['temperature'] * merged_df['humidity'] / 100\n",
    "    merged_df['wind_temp_interaction'] = merged_df['wind_speed'] * merged_df['temperature'] / 10\n",
    "    \n",
    "    # Create heating/cooling degree days\n",
    "    base_temp = 18  # Base temperature for HDD/CDD calculation (°C)\n",
    "    merged_df['heating_degree'] = np.maximum(0, base_temp - merged_df['temperature'])\n",
    "    merged_df['cooling_degree'] = np.maximum(0, merged_df['temperature'] - base_temp)\n",
    "    \n",
    "    # Create categorical season feature\n",
    "    merged_df['season'] = merged_df['month'].apply(lambda x: \n",
    "                                                   'Winter' if x in [12, 1, 2] else\n",
    "                                                   'Spring' if x in [3, 4, 5] else\n",
    "                                                   'Summer' if x in [6, 7, 8] else\n",
    "                                                   'Fall')\n",
    "    \n",
    "    # Add holiday flag (simplified - just weekends for demo)\n",
    "    merged_df['is_holiday'] = merged_df['is_weekend']\n",
    "    \n",
    "    # Create time block feature\n",
    "    merged_df['time_block'] = pd.cut(merged_df['hour'], \n",
    "                                     bins=[0, 6, 12, 18, 24], \n",
    "                                     labels=['night', 'morning', 'afternoon', 'evening'],\n",
    "                                     include_lowest=True, right=False)\n",
    "    \n",
    "    # Add peak/off-peak feature based on time of day\n",
    "    merged_df['is_peak_time'] = ((merged_df['hour'] >= 7) & (merged_df['hour'] <= 9) | \n",
    "                                  (merged_df['hour'] >= 17) & (merged_df['hour'] <= 20)).astype(int)\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    categorical_features = ['season', 'time_block']\n",
    "    merged_df = pd.get_dummies(merged_df, columns=categorical_features, drop_first=False)\n",
    "    \n",
    "    # Calculate energy price based on time of day (simulated)\n",
    "    peak_price = 0.22  # $/kWh during peak hours\n",
    "    off_peak_price = 0.12  # $/kWh during off-peak hours\n",
    "    \n",
    "    merged_df['energy_price'] = np.where(merged_df['is_peak_time'] == 1, peak_price, off_peak_price)\n",
    "    merged_df['energy_cost'] = merged_df['energy_consumption'] * merged_df['energy_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # Save processed data\n",
    "    merged_df.to_csv('data/processed_data.csv', index=False)\n",
    "    \n",
    "    print(f\"Final dataset shape: {merged_df.shape}\")\n",
    "    print(f\"Features: {', '.join(merged_df.columns.tolist())}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Merge datasets and engineer features\n",
    "final_df = merge_data_and_engineer_features(processed_energy_df, processed_weather_df)\n",
    "\n",
    "# Display data info\n",
    "print(\"\\nFinal Dataset Summary:\")\n",
    "print(final_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def perform_eda(df):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on the dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\nPerforming Exploratory Data Analysis...\")\n",
    "    \n",
    "    # Create figure for time series plot\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df['timestamp'], df['energy_consumption'])\n",
    "    plt.title('Energy Consumption Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Energy Consumption (kWh)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/energy_consumption_time_series.png')\n",
    "    \n",
    "    # Create daily and weekly patterns\n",
    "    # Aggregate by hour of day\n",
    "    hourly_avg = df.groupby('hour')['energy_consumption'].mean().reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(hourly_avg['hour'], hourly_avg['energy_consumption'])\n",
    "    plt.title('Average Energy Consumption by Hour of Day')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Avg Energy Consumption (kWh)')\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/hourly_energy_pattern.png')\n",
    "    \n",
    "    # Aggregate by day of week\n",
    "    weekly_avg = df.groupby('day_of_week')['energy_consumption'].mean().reset_index()\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(weekly_avg['day_of_week'], weekly_avg['energy_consumption'])\n",
    "    plt.title('Average Energy Consumption by Day of Week')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Avg Energy Consumption (kWh)')\n",
    "    plt.xticks(range(7), days)\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/weekly_energy_pattern.png')\n",
    "    \n",
    "    # Seasonal patterns\n",
    "    seasonal_avg = df.groupby('month')['energy_consumption'].mean().reset_index()\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(seasonal_avg['month'], seasonal_avg['energy_consumption'])\n",
    "    plt.title('Average Energy Consumption by Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Avg Energy Consumption (kWh)')\n",
    "    plt.xticks(range(1, 13), months)\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/monthly_energy_pattern.png')\n",
    "    \n",
    "    # Correlation with weather\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.scatter(df['temperature'], df['energy_consumption'], alpha=0.5)\n",
    "    plt.title('Energy Consumption vs. Temperature')\n",
    "    plt.xlabel('Temperature (°C)')\n",
    "    plt.ylabel('Energy Consumption (kWh)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/energy_vs_temperature.png')\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr_cols = ['energy_consumption', 'temperature', 'humidity', 'wind_speed', \n",
    "                 'cloud_cover', 'precipitation', 'hour', 'day_of_week',\n",
    "                 'is_weekend', 'heating_degree', 'cooling_degree']\n",
    "    \n",
    "    corr_matrix = df[corr_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/correlation_matrix.png')\n",
    "    \n",
    "    # Weekend vs Weekday comparison\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    weekend_data = df[df['is_weekend'] == 1].groupby('hour')['energy_consumption'].mean()\n",
    "    weekday_data = df[df['is_weekend'] == 0].groupby('hour')['energy_consumption'].mean()\n",
    "    \n",
    "    plt.plot(weekend_data.index, weekend_data.values, 'b-', linewidth=2, label='Weekend')\n",
    "    plt.plot(weekday_data.index, weekday_data.values, 'r-', linewidth=2, label='Weekday')\n",
    "    \n",
    "    plt.title('Weekend vs Weekday Energy Consumption Pattern')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Average Energy Consumption (kWh)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/weekend_vs_weekday.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # Print summary stats\n",
    "    print(\"\\nSummary Statistics by Season:\")\n",
    "    print(df.groupby('season_Winter')['energy_consumption'].describe())\n",
    "    \n",
    "    print(\"\\nCorrelation with Energy Consumption:\")\n",
    "    correlations = df.corr()['energy_consumption'].sort_values(ascending=False)\n",
    "    print(correlations.head(10))\n",
    "    print(correlations.tail(10))\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "# Perform EDA\n",
    "feature_correlations = perform_eda(final_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Selection and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def prepare_data_for_modeling(df, target_col='energy_consumption', test_size=0.2, validation_size=0.25):\n",
    "    \"\"\"\n",
    "    Prepare data for modeling:\n",
    "    - Split into training, validation, and test sets\n",
    "    - Scale numerical features\n",
    "    - Select important features\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing data for modeling...\")\n",
    "    \n",
    "    # Select features based on correlation analysis\n",
    "    # Exclude some columns that shouldn't be used for modeling\n",
    "    exclude_cols = ['timestamp', 'energy_price', 'energy_cost']\n",
    "    \n",
    "    # Get list of potentially useful features\n",
    "    features = [col for col in df.columns if col != target_col and col not in exclude_cols]\n",
    "    \n",
    "    # First split: training+validation vs test\n",
    "    train_val_df, test_df = train_test_split(df, test_size=test_size, shuffle=False)\n",
    "    \n",
    "    # Second split: training vs validation\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=validation_size, shuffle=False)\n",
    "    \n",
    "    # Get final feature list\n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df[target_col]\n",
    "    \n",
    "    X_val = val_df[features]\n",
    "    y_val = val_df[target_col]\n",
    "    \n",
    "    X_test = test_df[features]\n",
    "    y_test = test_df[target_col]\n",
    "    \n",
    "    # Define numerical features\n",
    "    numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Create preprocessor pipeline\n",
    "    preprocessor = StandardScaler()\n",
    "    \n",
    "    # Fit preprocessor on training data\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        preprocessor.fit_transform(X_train[numerical_features]),\n",
    "        columns=numerical_features,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    \n",
    "    # Transform validation and test data\n",
    "    X_val_scaled = pd.DataFrame(\n",
    "        preprocessor.transform(X_val[numerical_features]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        columns=numerical_features,\n",
    "        index=X_val.index\n",
    "    )\n",
    "    \n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        preprocessor.transform(X_test[numerical_features]),\n",
    "        columns=numerical_features,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    # Save scaler for later use\n",
    "    joblib.dump(preprocessor, 'models/scaler.pkl')\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, X_train_scaled, y_train, X_val, X_val_scaled, y_val, X_test, X_test_scaled, y_test, features, numerical_features\n",
    "\n",
    "# Prepare data for modeling\n",
    "X_train, X_train_scaled, y_train, X_val, X_val_scaled, y_val, X_test, X_test_scaled, y_test, features, numerical_features = prepare_data_for_modeling(final_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model performance.\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "\n",
    "def plot_predictions(y_true, y_pred, title, filename):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_true.index, y_true.values, 'b-', label='Actual')\n",
    "    plt.plot(y_true.index, y_pred, 'r-', label='Predicted')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Energy Consumption (kWh)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results/{filename}')\n",
    "\n",
    "# Function to create LSTM input sequences\n",
    "def create_sequences(X, y, time_steps=24):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM model.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:i+time_steps])\n",
    "        ys.append(y[i+time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# 1. Gradient Boosting Model\n",
    "def train_gradient_boosting(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train a Gradient Boosting Regressor.\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining Gradient Boosting model...\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [3, 5]\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    gb_model = GradientBoostingRegressor(random_state=42)\n",
    "    \n",
    "    # Create grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=gb_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best model\n",
    "    best_gb_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    val_predictions = best_gb_model.predict(X_val)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(y_val, val_predictions, \"Gradient Boosting\")\n",
    "    \n",
    "    # Plot predictions\n",
    "    plot_predictions(y_val, val_predictions, 'Gradient Boosting: Actual vs Predicted', 'gb_predictions.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_gb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance (Gradient Boosting):\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance.head(15))\n",
    "    plt.title('Feature Importance (Gradient Boosting)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/gb_feature_importance.png')\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(best_gb_model, 'models/gradient_boosting_model.pkl')\n",
    "    \n",
    "    return best_gb_model, metrics\n",
    "\n",
    "# 2. XGBoost Model\n",
    "def train_xgboost(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train an XGBoost model.\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining XGBoost model...\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "    \n",
    "    # Create grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best model\n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    val_predictions = best_xgb_model.predict(X_val)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(y_val, val_predictions, \"XGBoost\")\n",
    "    \n",
    "    # Plot predictions\n",
    "    plot_predictions(y_val,\n",
    "\n",
    "# Plot predictions\n",
    "    plot_predictions(y_val, val_predictions, 'XGBoost: Actual vs Predicted', 'xgb_predictions.png')\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance (XGBoost):\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance.head(15))\n",
    "    plt.title('Feature Importance (XGBoost)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/xgb_feature_importance.png')\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(best_xgb_model, 'models/xgboost_model.pkl')\n",
    "    \n",
    "    return best_xgb_model, metrics\n",
    "\n",
    "# 3. LightGBM Model\n",
    "def train_lightgbm(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train a LightGBM model.\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining LightGBM model...\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        'learning_rate': [0.05, 0.1],\n",
    "        'num_leaves': [31, 63],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "    \n",
    "    # Create grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=lgb_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best model\n",
    "    best_lgb_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    val_predictions = best_lgb_model.predict(X_val)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(y_val, val_predictions, \"LightGBM\")\n",
    "    \n",
    "    # Plot predictions\n",
    "    plot_predictions(y_val, val_predictions, 'LightGBM: Actual vs Predicted', 'lgb_predictions.png')\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_lgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance (LightGBM):\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(best_lgb_model, 'models/lightgbm_model.pkl')\n",
    "    \n",
    "    return best_lgb_model, metrics\n",
    "\n",
    "# 4. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(X_train_scaled, y_train, X_val_scaled, y_val, time_steps=24):\n",
    "    \"\"\"\n",
    "    Train an LSTM model for time series forecasting.\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining LSTM model...\")\n",
    "    \n",
    "    # Create sequences for LSTM\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled.values, y_train.values, time_steps)\n",
    "    X_val_seq, y_val_seq = create_sequences(X_val_scaled.values, y_val.values, time_steps)\n",
    "    \n",
    "    print(f\"LSTM sequence shapes: X_train={X_train_seq.shape}, y_train={y_train_seq.shape}\")\n",
    "    \n",
    "    # Build LSTM model\n",
    "    input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "    \n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='relu', input_shape=input_shape, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('LSTM Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/lstm_training_history.png')\n",
    "    \n",
    "    # Make predictions\n",
    "    val_predictions = model.predict(X_val_seq)\n",
    "    \n",
    "    # Original validation indices range\n",
    "    val_indices = y_val.index[time_steps:]\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(y_val[time_steps:], val_predictions.flatten(), \"LSTM\")\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(val_indices, y_val[time_steps:].values, 'b-', label='Actual')\n",
    "    plt.plot(val_indices, val_predictions.flatten(), 'r-', label='Predicted')\n",
    "    plt.title('LSTM: Actual vs Predicted')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Energy Consumption (kWh)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/lstm_predictions.png')\n",
    "    \n",
    "    # Save model\n",
    "    model.save('models/lstm_model.h5')\n",
    "    \n",
    "    # Save sequence parameters\n",
    "    with open('models/lstm_params.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'time_steps': time_steps,\n",
    "            'features': X_train_scaled.columns.tolist()\n",
    "        }, f)\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "# 5. Prophet Model\n",
    "def train_prophet_model(df, target_col='energy_consumption', exogenous_features=None):\n",
    "    \"\"\"\n",
    "    Train a Prophet model for time series forecasting.\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining Prophet model...\")\n",
    "    \n",
    "    # Prepare data for Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    prophet_df = df.reset_index()[['timestamp', target_col]].copy()\n",
    "    prophet_df.columns = ['ds', 'y']\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(len(prophet_df) * 0.8)\n",
    "    prophet_train = prophet_df.iloc[:train_size]\n",
    "    prophet_test = prophet_df.iloc[train_size:]\n",
    "    \n",
    "    # Create model\n",
    "    model = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=True,\n",
    "        changepoint_prior_scale=0.05\n",
    "    )\n",
    "    \n",
    "    # Add exogenous features if provided\n",
    "    if exogenous_features:\n",
    "        for feature in exogenous_features:\n",
    "            model.add_regressor(feature)\n",
    "            prophet_train[feature] = df[feature].iloc[:train_size].values\n",
    "            prophet_test[feature] = df[feature].iloc[train_size:].values\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(prophet_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    future = model.make_future_dataframe(periods=len(prophet_test), freq='H')\n",
    "    \n",
    "    # Add exogenous features to future dataframe\n",
    "    if exogenous_features:\n",
    "        for feature in exogenous_features:\n",
    "            future[feature] = df[feature].values\n",
    "    \n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # Get predictions for test period\n",
    "    test_predictions = forecast.iloc[-len(prophet_test):]['yhat'].values\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(prophet_test['y'].values, test_predictions, \"Prophet\")\n",
    "    \n",
    "    # Plot predictions\n",
    "    fig = model.plot(forecast)\n",
    "    plt.title('Prophet: Forecast vs Actual')\n",
    "    plt.savefig('results/prophet_predictions.png')\n",
    "    \n",
    "    # Plot components\n",
    "    fig = model.plot_components(forecast)\n",
    "    plt.savefig('results/prophet_components.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # Save model\n",
    "    with open('models/prophet_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "# Train all models\n",
    "gb_model, gb_metrics = train_gradient_boosting(X_train, y_train, X_val, y_val)\n",
    "xgb_model, xgb_metrics = train_xgboost(X_train, y_train, X_val, y_val)\n",
    "lgb_model, lgb_metrics = train_lightgbm(X_train, y_train, X_val, y_val)\n",
    "lstm_model, lstm_metrics = train_lstm_model(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "\n",
    "# Train Prophet model with some exogenous features\n",
    "exogenous_features = ['temperature', 'humidity', 'is_weekend']\n",
    "prophet_model, prophet_metrics = train_prophet_model(final_df, exogenous_features=exogenous_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Comparison & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def compare_models(metrics_list):\n",
    "    \"\"\"\n",
    "    Compare different models and select the best one.\n",
    "    \"\"\"\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_df = pd.DataFrame(metrics_list)\n",
    "    comparison_df = comparison_df.set_index('model_name')\n",
    "    \n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # RMSE comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    comparison_df['rmse'].plot(kind='bar')\n",
    "    plt.title('RMSE Comparison')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    # MAE comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    comparison_df['mae'].plot(kind='bar')\n",
    "    plt.title('MAE Comparison')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    # R² comparison\n",
    "    plt.subplot(2, 2, 3)\n",
    "    comparison_df['r2'].plot(kind='bar')\n",
    "    plt.title('R² Comparison')\n",
    "    plt.ylabel('R²')\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    # MAPE comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    comparison_df['mape'].plot(kind='bar')\n",
    "    plt.title('MAPE Comparison')\n",
    "    plt.ylabel('MAPE (%)')\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/model_comparison.png')\n",
    "    \n",
    "    # Find best model based on RMSE\n",
    "    best_model_name = comparison_df['rmse'].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(f\"\\nThe best model based on RMSE is: {best_model_name}\")\n",
    "    \n",
    "    return comparison_df, best_model_name\n",
    "\n",
    "# Compare all models\n",
    "metrics_list = [gb_metrics, xgb_metrics, lgb_metrics, lstm_metrics, prophet_metrics]\n",
    "comparison_df, best_model_name = compare_models(metrics_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Best Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def evaluate_best_model_on_test_set(best_model_name):\n",
    "    \"\"\"\n",
    "    Evaluate the best model on the test set.\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {best_model_name} on test set...\")\n",
    "    \n",
    "    # Load the appropriate model based on the name\n",
    "    if best_model_name == \"XGBoost\":\n",
    "        model = joblib.load('models/xgboost_model.pkl')\n",
    "        test_predictions = model.predict(X_test)\n",
    "        actual = y_test\n",
    "    elif best_model_name == \"LightGBM\":\n",
    "        model = joblib.load('models/lightgbm_model.pkl')\n",
    "        test_predictions = model.predict(X_test)\n",
    "        actual = y_test\n",
    "    elif best_model_name == \"Gradient Boosting\":\n",
    "        model = joblib.load('models/gradient_boosting_model.pkl')\n",
    "        test_predictions = model.predict(X_test)\n",
    "        actual = y_test\n",
    "    elif best_model_name == \"LSTM\":\n",
    "        model = tf.keras.models.load_model('models/lstm_model.h5')\n",
    "        # Load sequence parameters\n",
    "        with open('models/lstm_params.json', 'r') as f:\n",
    "            lstm_params = json.load(f)\n",
    "        \n",
    "        time_steps = lstm_params['time_steps']\n",
    "        X_test_seq, y_test_seq = create_sequences(X_test_scaled.values, y_test.values, time_steps)\n",
    "        test_predictions = model.predict(X_test_seq).flatten()\n",
    "        actual = y_test[time_steps:]\n",
    "    elif best_model_name == \"Prophet\":\n",
    "        with open('models/prophet_model.pkl', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        # This is simplified - in reality, you'd need proper data preparation for Prophet\n",
    "        test_predictions = prophet_metrics['predictions'] if 'predictions' in prophet_metrics else None\n",
    "        actual = y_test\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_metrics = evaluate_model(actual, test_predictions, f\"{best_model_name} (Test Set)\")\n",
    "    \n",
    "    # Plot test predictions\n",
    "    plot_predictions(actual, test_predictions, f'{best_model_name}: Test Set Predictions', 'best_model_test_predictions.png')\n",
    "    \n",
    "    return test_metrics\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model_test_metrics = evaluate_best_model_on_test_set(best_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def analyze_feature_importance(best_model_name):\n",
    "    \"\"\"\n",
    "    Analyze feature importance for the best model.\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing feature importance...\")\n",
    "    \n",
    "    if best_model_name in [\"XGBoost\", \"LightGBM\", \"Gradient Boosting\"]:\n",
    "        # Load the appropriate model\n",
    "        if best_model_name == \"XGBoost\":\n",
    "            model = joblib.load('models/xgboost_model.pkl')\n",
    "        elif best_model_name == \"LightGBM\":\n",
    "            model = joblib.load('models/lightgbm_model.pkl')\n",
    "        else:  # Gradient Boosting\n",
    "            model = joblib.load('models/gradient_boosting_model.pkl')\n",
    "        \n",
    "        # Get feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "        plt.title(f'Feature Importance ({best_model_name})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/best_model_feature_importance.png')\n",
    "        \n",
    "        print(\"\\nTop 20 Important Features:\")\n",
    "        print(feature_importance.head(20))\n",
    "        \n",
    "        return feature_importance\n",
    "    else:\n",
    "        print(f\"Feature importance analysis not implemented for {best_model_name}\")\n",
    "        return None\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance = analyze_feature_importance(best_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def create_prediction_pipeline(best_model_name):\n",
    "    \"\"\"\n",
    "    Create a prediction pipeline for the best model.\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating prediction pipeline...\")\n",
    "    \n",
    "    class EnergyPredictionPipeline:\n",
    "        \"\"\"\n",
    "        Energy prediction pipeline for real-time predictions.\n",
    "        \"\"\"\n",
    "        def __init__(self, model_path, scaler_path=None, model_type=None, lstm_params_path=None):\n",
    "            self.model_type = model_type\n",
    "            \n",
    "            # Load model\n",
    "            if model_type in [\"XGBoost\", \"LightGBM\", \"Gradient Boosting\"]:\n",
    "                self.model = joblib.load(model_path)\n",
    "            elif model_type == \"LSTM\":\n",
    "                self.model = tf.keras.models.load_model(model_path)\n",
    "                # Load LSTM parameters\n",
    "                with open(lstm_params_path, 'r') as f:\n",
    "                    lstm_params = json.load(f)\n",
    "                self.time_steps = lstm_params['time_steps']\n",
    "                self.features = lstm_params['features']\n",
    "            elif model_type == \"Prophet\":\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    self.model = pickle.load(f)\n",
    "            \n",
    "            # Load scaler if needed\n",
    "            if scaler_path and model_type in [\"XGBoost\", \"LightGBM\", \"Gradient Boosting\", \"LSTM\"]:\n",
    "                self.scaler = joblib.load(scaler_path)\n",
    "            \n",
    "        def preprocess_data(self, df):\n",
    "            \"\"\"\n",
    "            Preprocess input data.\n",
    "            \"\"\"\n",
    "            # This should implement the same preprocessing steps as in the training pipeline\n",
    "            # For demonstration, we'll assume df is already preprocessed\n",
    "            return df\n",
    "        \n",
    "        def predict(self, df):\n",
    "            \"\"\"\n",
    "            Make predictions using the loaded model.\n",
    "            \"\"\"\n",
    "            # Preprocess data\n",
    "            processed_df = self.preprocess_data(df)\n",
    "            \n",
    "            if self.model_type in [\"XGBoost\", \"LightGBM\", \"Gradient Boosting\"]:\n",
    "                # Scale numerical features if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                if hasattr(self, 'scaler'):\n",
    "                    numerical_features = processed_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "                    processed_df[numerical_features] = self.scaler.transform(processed_df[numerical_features])\n",
    "                \n",
    "                # Make predictions\n",
    "                predictions = self.model.predict(processed_df)\n",
    "                \n",
    "            elif self.model_type == \"LSTM\":\n",
    "                # Scale data\n",
    "                numerical_features = processed_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "                scaled_data = self.scaler.transform(processed_df[numerical_features])\n",
    "                \n",
    "                # Create sequences\n",
    "                sequences = []\n",
    "                for i in range(len(scaled_data) - self.time_steps + 1):\n",
    "                    sequences.append(scaled_data[i:i+self.time_steps])\n",
    "                \n",
    "                # Convert to numpy array\n",
    "                sequences = np.array(sequences)\n",
    "                \n",
    "                # Make predictions\n",
    "                predictions = self.model.predict(sequences).flatten()\n",
    "                \n",
    "                # Adjust predictions length to match input length\n",
    "                # Pad with NaN at the beginning\n",
    "                padding = np.full(self.time_steps - 1, np.nan)\n",
    "                predictions = np.concatenate([padding, predictions])\n",
    "                \n",
    "            elif self.model_type == \"Prophet\":\n",
    "                # Prepare data for Prophet\n",
    "                prophet_df = processed_df.reset_index()[['timestamp']].copy()\n",
    "                prophet_df.columns = ['ds']\n",
    "                \n",
    "                # Make predictions\n",
    "                forecast = self.model.predict(prophet_df)\n",
    "                predictions = forecast['yhat'].values\n",
    "            \n",
    "            return predictions\n",
    "    \n",
    "    # Create the appropriate pipeline based on the best model\n",
    "    if best_model_name in [\"XGBoost\", \"LightGBM\", \"Gradient Boosting\"]:\n",
    "        model_path = f'models/{best_model_name.lower().replace(\" \", \"_\")}_model.pkl'\n",
    "    elif best_model_name == \"LSTM\":\n",
    "        model_path = 'models/lstm_model.h5'\n",
    "    else:  # Prophet\n",
    "        model_path = 'models/prophet_model.pkl'\n",
    "    \n",
    "    # Create the pipeline instance\n",
    "    if best_model_name == \"LSTM\":\n",
    "        pipeline = EnergyPredictionPipeline("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            model_path=model_path,\n",
    "            scaler_path='models/scaler.pkl',\n",
    "            model_type=best_model_name,\n",
    "            lstm_params_path='models/lstm_params.json'\n",
    "        )\n",
    "    else:\n",
    "        pipeline = EnergyPredictionPipeline(\n",
    "            model_path=model_path,\n",
    "            scaler_path='models/scaler.pkl',\n",
    "            model_type=best_model_name\n",
    "        )\n",
    "    \n",
    "    # Save the pipeline class definition\n",
    "    with open('models/prediction_pipeline.py', 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "class EnergyPredictionPipeline:\n",
    "    \\\"\\\"\\\"\n",
    "    Energy prediction pipeline for real-time predictions.\n",
    "    \\\"\\\"\\\"\n",
    "    def __init__(self, model_path, scaler_path=None, model_type=None, lstm_params_path=None):\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        # Load model\n",
    "        if model_type in [\"XGBoost\", \"LightGBM\", \"Gradient Boosting\"]:\n",
    "            self.model = joblib.load(model_path)\n",
    "        elif model_type == \"LSTM\":\n",
    "            self.model = tf.keras.models.load_model(model_path)\n",
    "            # Load LSTM parameters\n",
    "            with open(lstm_params_path, 'r') as f:\n",
    "                lstm_params = json.load(f)\n",
    "            self.time_steps = lstm_params['time_steps']\n",
    "            self.features = lstm_params['features']\n",
    "        elif model_type == \"Prophet\":\n",
    "            with open(model_path, 'rb') as f:\n",
    "                self.model = pickle.load(f)\n",
    "        \n",
    "        # Load scaler if needed\n",
    "        if scaler_path and model_type in [\"XGBoost\", \"LightGBM\", \"Gradient Boosting\", \"LSTM\"]:\n",
    "            self.scaler = joblib.load(scaler_path)\n",
    "        \n",
    "    def preprocess_data(self, df):\n",
    "        \\\"\\\"\\\"\n",
    "        Preprocess input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \\\"\\\"\\\"\n",
    "        # This should implement the same preprocessing steps as in the training pipeline\n",
    "        # For demonstration, we'll assume df is already preprocessed\n",
    "        return df\n",
    "    \n",
    "    def predict(self, df):\n",
    "        \\\"\\\"\\\"\n",
    "        Make predictions using the loaded model.\n",
    "        \\\"\\\"\\\"\n",
    "        # Preprocess data\n",
    "        processed_df = self.preprocess_data(df)\n",
    "        \n",
    "        if self.model_type in [\"XGBoost\", \"LightGBM\", \"Gradient Boosting\"]:\n",
    "            # Scale numerical features if needed\n",
    "            if hasattr(self, 'scaler'):\n",
    "                numerical_features = processed_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "                processed_df[numerical_features] = self.scaler.transform(processed_df[numerical_features])\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = self.model.predict(processed_df)\n",
    "            \n",
    "        elif self.model_type == \"LSTM\":\n",
    "            # Scale data\n",
    "            numerical_features = processed_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "            scaled_data = self.scaler.transform(processed_df[numerical_features])\n",
    "            \n",
    "            # Create sequences\n",
    "            sequences = []\n",
    "            for i in range(len(scaled_data) - self.time_steps + 1):\n",
    "                sequences.append(scaled_data[i:i+self.time_steps])\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            sequences = np.array(sequences)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = self.model.predict(sequences).flatten()\n",
    "            \n",
    "            # Adjust predictions length to match input length\n",
    "            # Pad with NaN at the beginning\n",
    "            padding = np.full(self.time_steps - 1, np.nan)\n",
    "            predictions = np.concatenate([padding, predictions])\n",
    "            \n",
    "        elif self.model_type == \"Prophet\":\n",
    "            # Prepare data for Prophet\n",
    "            prophet_df = processed_df.reset_index()[['timestamp']].copy()\n",
    "            prophet_df.columns = ['ds']\n",
    "            \n",
    "            # Make predictions\n",
    "            forecast = self.model.predict(prophet_df)\n",
    "            predictions = forecast['yhat'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        return predictions\n",
    "\n",
    "def load_pipeline():\n",
    "    \\\"\\\"\\\"\n",
    "    Load the prediction pipeline.\n",
    "    \\\"\\\"\\\"\n",
    "    best_model_name = \"{}\"\n",
    "    \n",
    "    if best_model_name in [\"XGBoost\", \"LightGBM\", \"Gradient Boosting\"]:\n",
    "        model_path = f'models/{{best_model_name.lower().replace(\" \", \"_\")}}_model.pkl'\n",
    "    elif best_model_name == \"LSTM\":\n",
    "        model_path = 'models/lstm_model.h5'\n",
    "    else:  # Prophet\n",
    "        model_path = 'models/prophet_model.pkl'\n",
    "    \n",
    "    if best_model_name == \"LSTM\":\n",
    "        pipeline = EnergyPredictionPipeline(\n",
    "            model_path=model_path,\n",
    "            scaler_path='models/scaler.pkl',\n",
    "            model_type=best_model_name,\n",
    "            lstm_params_path='models/lstm_params.json'\n",
    "        )\n",
    "    else:\n",
    "        pipeline = EnergyPredictionPipeline(\n",
    "            model_path=model_path,\n",
    "            scaler_path='models/scaler.pkl',\n",
    "            model_type=best_model_name\n",
    "        )\n",
    "    \n",
    "    return pipeline\n",
    "\"\"\".format(best_model_name))\n",
    "\n",
    "    print(f\"Prediction pipeline created for {best_model_name} model.\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Create prediction pipeline\n",
    "pipeline = create_prediction_pipeline(best_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Create Flask API for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def create_flask_api():\n",
    "    \"\"\"\n",
    "    Create a Flask API for the energy prediction model.\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating Flask API for deployment...\")\n",
    "    \n",
    "    with open('app.py', 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "from models.prediction_pipeline import load_pipeline\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the prediction pipeline\n",
    "pipeline = load_pipeline()\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    return jsonify({'status': 'healthy'})\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Get input data\n",
    "        data = request.json\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        if 'features' in data:\n",
    "            # Direct feature input\n",
    "            df = pd.DataFrame(data['features'])\n",
    "        elif 'timestamp' in data:\n",
    "            # Time-based prediction\n",
    "            timestamps = data['timestamp']\n",
    "            if not isinstance(timestamps, list):\n",
    "                timestamps = [timestamps]\n",
    "            \n",
    "            # Convert to datetime\n",
    "            timestamps = [datetime.fromisoformat(ts.replace('Z', '+00:00')) for ts in timestamps]\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame({'timestamp': timestamps})\n",
    "            \n",
    "            # Add weather data if provided\n",
    "            if 'weather' in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                for key, values in data['weather'].items():\n",
    "                    df[key] = values\n",
    "        else:\n",
    "            return jsonify({'error': 'Invalid input format'}), 400\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = pipeline.predict(df)\n",
    "        \n",
    "        # Convert predictions to list\n",
    "        predictions = predictions.tolist()\n",
    "        \n",
    "        # Create response\n",
    "        response = {\n",
    "            'predictions': predictions,\n",
    "            'timestamp': df['timestamp'].astype(str).tolist() if 'timestamp' in df else None\n",
    "        }\n",
    "        \n",
    "        return jsonify(response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/batch_predict', methods=['POST'])\n",
    "def batch_predict():\n",
    "    try:\n",
    "        # Get input data\n",
    "        file = request.files['file']\n",
    "        \n",
    "        # Read CSV file\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Convert timestamp to datetime if present\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = pipeline.predict(df)\n",
    "        \n",
    "        # Create response\n",
    "        response = {\n",
    "            'predictions': predictions.tolist(),\n",
    "            'timestamp': df['timestamp'].astype(str).tolist() if 'timestamp' in df else None\n",
    "        }\n",
    "        \n",
    "        return jsonify(response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    app.run(debug=True, host='0.0.0.0', port=5000)\n",
    "\"\"\")\n",
    "    \n",
    "    # Create requirements.txt file\n",
    "    with open('requirements.txt', 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "numpy==1.24.3\n",
    "pandas==2.0.3\n",
    "scikit-learn==1.3.0\n",
    "tensorflow==2.14.0\n",
    "xgboost==1.7.6\n",
    "lightgbm==4.0.0\n",
    "prophet==1.1.4\n",
    "matplotlib==3.7.2\n",
    "seaborn==0.12.2\n",
    "flask==2.3.3\n",
    "gunicorn==21.2.0\n",
    "joblib==1.3.2\n",
    "\"\"\")\n",
    "    \n",
    "    # Create Dockerfile\n",
    "    with open('Dockerfile', 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 5000\n",
    "\n",
    "CMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"app:app\"]\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"Flask API, requirements.txt, and Dockerfile created for deployment.\")\n",
    "\n",
    "# Create Flask API\n",
    "create_flask_api()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Energy Savings Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def create_energy_savings_calculator():\n",
    "    \"\"\"\n",
    "    Create an energy savings calculator.\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating energy savings calculator...\")\n",
    "    \n",
    "    with open('energy_savings_calculator.py', 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models.prediction_pipeline import load_pipeline\n",
    "\n",
    "class EnergySavingsCalculator:\n",
    "    \\\"\\\"\\\"\n",
    "    A calculator for estimating energy savings based on different scenarios.\n",
    "    \\\"\\\"\\\"\n",
    "    def __init__(self, pipeline=None):\n",
    "        if pipeline is None:\n",
    "            self.pipeline = load_pipeline()\n",
    "        else:\n",
    "            self.pipeline = pipeline\n",
    "        \n",
    "        # Default energy price ($/kWh)\n",
    "        self.peak_price = 0.22\n",
    "        self.off_peak_price = 0.12\n",
    "        \n",
    "        # Peak hours (7-9 AM and 5-8 PM)\n",
    "        self.peak_hours = list(range(7, 10)) + list(range(17, 21))\n",
    "    \n",
    "    def calculate_baseline_consumption(self, df):\n",
    "        \\\"\\\"\\\"\n",
    "        Calculate baseline energy consumption.\n",
    "        \\\"\\\"\\\"\n",
    "        # Make predictions using the trained model\n",
    "        baseline_predictions = self.pipeline.predict(df)\n",
    "        \n",
    "        # Calculate total consumption\n",
    "        total_consumption = np.nansum(baseline_predictions)\n",
    "        \n",
    "        # Calculate cost\n",
    "        cost = self._calculate_cost(df, baseline_predictions)\n",
    "        \n",
    "        return {\n",
    "            'predictions': baseline_predictions,\n",
    "            'total_consumption': total_consumption,\n",
    "            'cost': cost\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def calculate_scenario_consumption(self, df, scenario):\n",
    "\n",
    "def calculate_scenario_consumption(self, df, scenario):\n",
    "        \"\"\"\n",
    "        Calculate energy consumption for a specific scenario.\n",
    "        \n",
    "        Scenarios:\n",
    "        - 'temperature_reduction': Reduce temperature by 2°C during summer months\n",
    "        - 'smart_scheduling': Shift energy usage from peak to off-peak hours\n",
    "        - 'efficient_appliances': Reduce overall consumption by 15%\n",
    "        \"\"\"\n",
    "        # Create a copy of the input data\n",
    "        scenario_df = df.copy()\n",
    "        \n",
    "        if scenario == 'temperature_reduction':\n",
    "            # Only apply during summer months (6-9)\n",
    "            if 'month' in scenario_df.columns:\n",
    "                summer_mask = scenario_df['month'].isin([6, 7, 8, 9])\n",
    "                if 'temperature' in scenario_df.columns:\n",
    "                    # Reduce temperature setpoint by 2°C\n",
    "                    scenario_df.loc[summer_mask, 'temperature'] += 2\n",
    "        \n",
    "        elif scenario == 'smart_scheduling':\n",
    "            # Shift energy from peak to off-peak hours\n",
    "            # This is simplified - in reality, you'd need to implement more complex logic\n",
    "            if 'hour' in scenario_df.columns:\n",
    "                peak_mask = scenario_df['hour'].isin(self.peak_hours)\n",
    "                # Add a feature indicating smart scheduling is active\n",
    "                scenario_df['smart_scheduling'] = peak_mask.astype(int)\n",
    "        \n",
    "        elif scenario == 'efficient_appliances':\n",
    "            # For this scenario, we'll assume a post-prediction adjustment\n",
    "            pass\n",
    "        \n",
    "        # Make predictions\n",
    "        scenario_predictions = self.pipeline.predict(scenario_df)\n",
    "        \n",
    "        # Apply post-prediction adjustments\n",
    "        if scenario == 'efficient_appliances':\n",
    "            # 15% reduction in energy consumption\n",
    "            scenario_predictions = scenario_predictions * 0.85\n",
    "        elif scenario == 'smart_scheduling':\n",
    "            # Shift 20% of peak consumption to off-peak\n",
    "            if 'hour' in scenario_df.columns:\n",
    "                peak_mask = scenario_df['hour'].isin(self.peak_hours)\n",
    "                peak_reduction = scenario_predictions[peak_mask] * 0.2\n",
    "                \n",
    "                # Distribute this to off-peak hours\n",
    "                off_peak_mask = ~peak_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                if np.sum(off_peak_mask) > 0:  # Make sure we have off-peak hours\n",
    "                    avg_increase = np.sum(peak_reduction) / np.sum(off_peak_mask)\n",
    "                    scenario_predictions[off_peak_mask] += avg_increase\n",
    "                    scenario_predictions[peak_mask] -= peak_reduction\n",
    "        \n",
    "        # Calculate total consumption\n",
    "        total_consumption = np.nansum(scenario_predictions)\n",
    "        \n",
    "        # Calculate cost\n",
    "        cost = self._calculate_cost(scenario_df, scenario_predictions)\n",
    "        \n",
    "        return {\n",
    "            'predictions': scenario_predictions,\n",
    "            'total_consumption': total_consumption,\n",
    "            'cost': cost\n",
    "        }\n",
    "    \n",
    "    def _calculate_cost(self, df, predictions):\n",
    "        \"\"\"\n",
    "        Calculate energy cost based on time-of-use pricing.\n",
    "        \"\"\"\n",
    "        if 'hour' not in df.columns:\n",
    "            # If hour is not available, use average price\n",
    "            return np.nansum(predictions) * ((self.peak_price + self.off_peak_price) / 2)\n",
    "        \n",
    "        # Create peak/off-peak mask\n",
    "        peak_mask = df['hour'].isin(self.peak_hours)\n",
    "        \n",
    "        # Calculate cost\n",
    "        peak_cost = np.nansum(predictions[peak_mask]) * self.peak_price\n",
    "        off_peak_cost = np.nansum(predictions[~peak_mask]) * self.off_peak_price\n",
    "        \n",
    "        return peak_cost + off_peak_cost\n",
    "    \n",
    "    def compare_scenarios(self, df, scenarios=None):\n",
    "        \"\"\"\n",
    "        Compare different energy-saving scenarios.\n",
    "        \"\"\"\n",
    "        if scenarios is None:\n",
    "            scenarios = ['temperature_reduction', 'smart_scheduling', 'efficient_appliances']\n",
    "        \n",
    "        # Calculate baseline\n",
    "        baseline = self.calculate_baseline_consumption(df)\n",
    "        \n",
    "        results = {'baseline': baseline}\n",
    "        \n",
    "        # Calculate scenarios\n",
    "        for scenario in scenarios:\n",
    "            results[scenario] = self.calculate_scenario_consumption(df, scenario)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Calculate savings\n",
    "        for scenario in scenarios:\n",
    "            consumption_savings = baseline['total_consumption'] - results[scenario]['total_consumption']\n",
    "            cost_savings = baseline['cost'] - results[scenario]['cost']\n",
    "            \n",
    "            results[scenario]['consumption_savings'] = consumption_savings\n",
    "            results[scenario]['cost_savings'] = cost_savings\n",
    "            results[scenario]['consumption_savings_pct'] = (consumption_savings / baseline['total_consumption']) * 100\n",
    "            results[scenario]['cost_savings_pct'] = (cost_savings / baseline['cost']) * 100\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_comparison(self, results):\n",
    "        \"\"\"\n",
    "        Plot comparison of different scenarios.\n",
    "        \"\"\"\n",
    "        scenarios = list(results.keys())\n",
    "        \n",
    "        # Plot consumption comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        consumption_values = [results[s]['total_consumption'] for s in scenarios]\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(scenarios, consumption_values)\n",
    "        plt.title('Total Energy Consumption')\n",
    "        plt.ylabel('Energy (kWh)')\n",
    "        plt.grid(axis='y')\n",
    "        \n",
    "        # Plot cost comparison\n",
    "        plt.subplot(1, 2, 2)\n",
    "        cost_values = [results[s]['cost'] for s in scenarios]\n",
    "        \n",
    "        plt.bar(scenarios, cost_values)\n",
    "        plt.title('Total Energy Cost')\n",
    "        plt.ylabel('Cost ($)')\n",
    "        plt.grid(axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/scenario_comparison.png')\n",
    "        \n",
    "        # Plot savings percentages\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        savings_scenarios = [s for s in scenarios if s != 'baseline']\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        consumption_savings = [results[s]['consumption_savings_pct'] for s in savings_scenarios]\n",
    "        \n",
    "        plt.bar(savings_scenarios, consumption_savings)\n",
    "        plt.title('Energy Savings (%)')\n",
    "        plt.ylabel('Savings (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        plt.grid(axis='y')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        cost_savings = [results[s]['cost_savings_pct'] for s in savings_scenarios]\n",
    "        \n",
    "        plt.bar(savings_scenarios, cost_savings)\n",
    "        plt.title('Cost Savings (%)')\n",
    "        plt.ylabel('Savings (%)')\n",
    "        plt.grid(axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/savings_comparison.png')\n",
    "        \n",
    "        return plt\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"Energy savings calculator created.\")\n",
    "\n",
    "# Create energy savings calculator\n",
    "create_energy_savings_calculator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Create Dashboard Integration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def create_dashboard_integration():\n",
    "    \"\"\"\n",
    "    Create a simple dashboard integration example.\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating dashboard integration example...\")\n",
    "    \n",
    "    with open('dashboard_integration.py', 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from models.prediction_pipeline import load_pipeline\n",
    "from energy_savings_calculator import EnergySavingsCalculator\n",
    "\n",
    "class EnergyDashboard:\n",
    "    \\\"\\\"\\\"\n",
    "    A simple dashboard integration example.\n",
    "    \\\"\\\"\\\"\n",
    "    def __init__(self):\n",
    "        # Load pipeline\n",
    "        self.pipeline = load_pipeline()\n",
    "        \n",
    "        # Create energy savings calculator\n",
    "        self.calculator = EnergySavingsCalculator(self.pipeline)\n",
    "    \n",
    "    def get_recent_data(self, days=7):\n",
    "        \\\"\\\"\\\"\n",
    "        Get recent energy and weather data.\n",
    "        This is a simulation - in a real scenario, this would fetch actual data.\n",
    "        \\\"\\\"\\\"\n",
    "        # Create date range\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "        \n",
    "        # Create hourly timestamps\n",
    "        timestamps = pd.date_range(start=start_date, end=end_date, freq='1h')\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({'timestamp': timestamps})\n",
    "        \n",
    "        # Add datetime features\n",
    "        df['hour'] = df['timestamp'].dt.hour\n",
    "        df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "        df['month'] = df['timestamp'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        # Add simulated weather data\n",
    "        df['temperature'] = 20 + 5 * np.sin(np.linspace(0, days * 2 * np.pi, len(df)))\n",
    "        df['humidity'] = 60 + 20 * np.random.rand(len(df))\n",
    "        df['wind_speed'] = 5 + 3 * np.random.rand(len(df))\n",
    "        df['cloud_cover'] = 50 + 30 * np.random.rand(len(df))\n",
    "        df['precipitation'] = np.random.exponential(scale=0.5, size=len(df)) * np.random.choice([0, 1], size=len(df), p=[0.8, 0.2])\n",
    "        \n",
    "        # Add derived weather features\n",
    "        df['feels_like'] = df['temperature'] - 0.4 * (df['wind_speed'] / 10) * (33 - df['temperature'])\n",
    "        df['heat_index'] = df['temperature'] + 0.05 * df['humidity']\n",
    "        \n",
    "        # Create weather condition categories\n",
    "        df['is_rainy'] = (df['precipitation'] > 0.5).astype(int)\n",
    "        df['is_windy'] = (df['wind_speed'] > 10).astype(int)\n",
    "        df['is_humid'] = (df['humidity'] > 70).astype(int)\n",
    "        df['is_cold'] = (df['temperature'] < 10).astype(int)\n",
    "        df['is_hot'] = (df['temperature'] > 25).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_predictions(self, df=None, days_ahead=1):\n",
    "        \\\"\\\"\\\"\n",
    "        Get energy consumption predictions.\n",
    "        \\\"\\\"\\\"\n",
    "        if df is None:\n",
    "            df = self.get_recent_data()\n",
    "        \n",
    "        # Make predictions for current data\n",
    "        current_predictions = self.pipeline.predict(df)\n",
    "        \n",
    "        # Create future data for forecasting\n",
    "        last_timestamp = df['timestamp'].iloc[-1]\n",
    "        future_start = last_timestamp + timedelta(hours=1)\n",
    "        future_end = future_start + timedelta(days=days_ahead)\n",
    "        \n",
    "        future_timestamps = pd.date_range(start=future_start, end=future_end, freq='1h')\n",
    "        future_df = pd.DataFrame({'timestamp': future_timestamps})\n",
    "        \n",
    "        # Add datetime features\n",
    "        future_df['hour'] = future_df['timestamp'].dt.hour\n",
    "        future_df['day_of_week'] = future_df['timestamp'].dt.dayofweek\n",
    "        future_df['month'] = future_df['timestamp'].dt.month\n",
    "        future_df['is_weekend'] = future_df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        # Add simulated weather data (simplified forecast)\n",
    "        future_df['temperature'] = 20 + 5 * np.sin(np.linspace(0, days_ahead * 2 * np.pi, len(future_df)))\n",
    "        future_df['humidity'] = 60 + 20 * np.random.rand(len(future_df))\n",
    "        future_df['wind_speed'] = 5 + 3 * np.random.rand(len(future_df))\n",
    "        future_df['cloud_cover'] = 50 + 30 * np.random.rand(len(future_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        future_df['precipitation'] = np.random.exponential(scale=0.5, size=len(future_df)) * np.random.choice([0, 1], size=len(future_df), p=[0.8, 0.2])\n",
    "        \n",
    "        # Add derived weather features\n",
    "        future_df['feels_like'] = future_df['temperature'] - 0.4 * (future_df['wind_speed'] / 10) * (33 - future_df['temperature'])\n",
    "        future_df['heat_index'] = future_df['temperature'] + 0.05 * future_df['humidity']\n",
    "        \n",
    "        # Create weather condition categories\n",
    "        future_df['is_rainy'] = (future_df['precipitation'] > 0.5).astype(int)\n",
    "        future_df['is_windy'] = (future_df['wind_speed'] > 10).astype(int)\n",
    "        future_df['is_humid'] = (future_df['humidity'] > 70).astype(int)\n",
    "        future_df['is_cold'] = (future_df['temperature'] < 10).astype(int)\n",
    "        future_df['is_hot'] = (future_df['temperature'] > 25).astype(int)\n",
    "        \n",
    "        # Make future predictions\n",
    "        future_predictions = self.pipeline.predict(future_df)\n",
    "        \n",
    "        return {\n",
    "            'current': {\n",
    "                'data': df,\n",
    "                'predictions': current_predictions\n",
    "            },\n",
    "            'future': {\n",
    "                'data': future_df,\n",
    "                'predictions': future_predictions\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_savings_opportunities(self, df=None):\n",
    "        \\\"\\\"\\\"\n",
    "        Get energy savings opportunities.\n",
    "        \\\"\\\"\\\"\n",
    "        if df is None:\n",
    "            df = self.get_recent_data()\n",
    "            \n",
    "        # Extend data for future predictions\n",
    "        future_days = 30  # One month forecast\n",
    "        predictions = self.get_predictions(df, days_ahead=future_days)\n",
    "        future_df = predictions['future']['data']\n",
    "        \n",
    "        # Compare different scenarios\n",
    "        scenarios = ['temperature_reduction', 'smart_scheduling', 'efficient_appliances']\n",
    "        results = self.calculator.compare_scenarios(future_df, scenarios)\n",
    "        \n",
    "        # Create summary\n",
    "        summary = {}\n",
    "        for scenario in scenarios:\n",
    "            summary[scenario] = {\n",
    "                'consumption_savings_kwh': float(results[scenario]['consumption_savings']),\n",
    "                'consumption_savings_pct': float(results[scenario]['consumption_savings_pct']),\n",
    "                'cost_savings_usd': float(results[scenario]['cost_savings']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                'cost_savings_pct': float(results[scenario]['cost_savings_pct'])\n",
    "            }\n",
    "        \n",
    "        # Plot comparison\n",
    "        self.calculator.plot_comparison(results)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def get_dashboard_data(self):\n",
    "        \\\"\\\"\\\"\n",
    "        Get all data needed for the dashboard.\n",
    "        \\\"\\\"\\\"\n",
    "        # Get recent data\n",
    "        recent_data = self.get_recent_data(days=7)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.get_predictions(recent_data, days_ahead=7)\n",
    "        \n",
    "        # Get savings opportunities\n",
    "        savings = self.get_savings_opportunities(recent_data)\n",
    "        \n",
    "        # Calculate current day's consumption\n",
    "        today = datetime.now().date()\n",
    "        today_mask = recent_data['timestamp'].dt.date == today\n",
    "        today_data = recent_data[today_mask]\n",
    "        \n",
    "        if len(today_data) > 0:\n",
    "            today_predictions = predictions['current']['predictions'][today_mask]\n",
    "            today_consumption = np.nansum(today_predictions)\n",
    "        else:\n",
    "            today_consumption = 0\n",
    "        \n",
    "        # Calculate yesterday's consumption\n",
    "        yesterday = today - timedelta(days=1)\n",
    "        yesterday_mask = recent_data['timestamp'].dt.date == yesterday\n",
    "        yesterday_data = recent_data[yesterday_mask]\n",
    "        \n",
    "        if len(yesterday_data) > 0:\n",
    "            yesterday_predictions = predictions['current']['predictions'][yesterday_mask]\n",
    "            yesterday_consumption = np.nansum(yesterday_predictions)\n",
    "        else:\n",
    "            yesterday_consumption = 0\n",
    "        \n",
    "        # Calculate week's consumption\n",
    "        week_consumption = np.nansum(predictions['current']['predictions'])\n",
    "        \n",
    "        # Calculate month-to-date consumption\n",
    "        month_start = datetime(today.year, today.month, 1).date()\n",
    "        month_mask = recent_data['timestamp'].dt.date >= month_start\n",
    "        month_data = recent_data[month_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        if len(month_data) > 0:\n",
    "            month_predictions = predictions['current']['predictions'][month_mask]\n",
    "            month_consumption = np.nansum(month_predictions)\n",
    "        else:\n",
    "            month_consumption = 0\n",
    "        \n",
    "        # Create dashboard data\n",
    "        dashboard_data = {\n",
    "            'current_stats': {\n",
    "                'today_consumption': float(today_consumption),\n",
    "                'yesterday_consumption': float(yesterday_consumption),\n",
    "                'week_consumption': float(week_consumption),\n",
    "                'month_consumption': float(month_consumption),\n",
    "                'day_over_day_change': float((today_consumption - yesterday_consumption) / yesterday_consumption * 100) if yesterday_consumption > 0 else 0\n",
    "            },\n",
    "            'predictions': {\n",
    "                'timestamps': predictions['future']['data']['timestamp'].astype(str).tolist(),\n",
    "                'values': predictions['future']['predictions'].tolist()\n",
    "            },\n",
    "            'recent': {\n",
    "                'timestamps': predictions['current']['data']['timestamp'].astype(str).tolist(),\n",
    "                'values': predictions['current']['predictions'].tolist()\n",
    "            },\n",
    "            'savings_opportunities': savings\n",
    "        }\n",
    "        \n",
    "        # Save dashboard data\n",
    "        with open('results/dashboard_data.json', 'w') as f:\n",
    "            json.dump(dashboard_data, f, indent=2)\n",
    "        \n",
    "        return dashboard_data\n",
    "    \n",
    "    def plot_dashboard_preview(self):\n",
    "        \\\"\\\"\\\"\n",
    "        Create a preview of the dashboard.\n",
    "        \\\"\\\"\\\"\n",
    "        # Get dashboard data\n",
    "        data = self.get_dashboard_data()\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        \n",
    "        # Plot recent consumption\n",
    "        plt.subplot(3, 1, 1)\n",
    "        recent_timestamps = pd.to_datetime(data['recent']['timestamps'])\n",
    "        plt.plot(recent_timestamps, data['recent']['values'])\n",
    "        plt.title('Recent Energy Consumption')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Energy (kWh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot predicted consumption\n",
    "        plt.subplot(3, 1, 2)\n",
    "        future_timestamps = pd.to_datetime(data['predictions']['timestamps'])\n",
    "        plt.plot(future_timestamps, data['predictions']['values'], 'r--')\n",
    "        plt.title('Forecasted Energy Consumption')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Energy (kWh)')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot savings opportunities\n",
    "        plt.subplot(3, 1, 3)\n",
    "        scenarios = list(data['savings_opportunities'].keys())\n",
    "        savings_values = [data['savings_opportunities'][s]['cost_savings_usd'] for s in scenarios]\n",
    "        \n",
    "        plt.bar(scenarios, savings_values)\n",
    "        plt.title('Monthly Savings Opportunities ($)')\n",
    "        plt.ylabel('Savings ($)')\n",
    "        plt.grid(axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/dashboard_preview.png')\n",
    "        \n",
    "        return plt\n",
    "\n",
    "# Create a function to generate the dashboard\n",
    "def generate_dashboard():\n",
    "    dashboard = EnergyDashboard()\n",
    "    data = dashboard.get_dashboard_data()\n",
    "    dashboard.plot_dashboard_preview()\n",
    "    print(f\"Dashboard data saved to 'results/dashboard_data.json'\")\n",
    "    print(f\"Dashboard preview saved to 'results/dashboard_preview.png'\")\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_dashboard()\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"Dashboard integration example created.\")\n",
    "\n",
    "# Create dashboard integration\n",
    "create_dashboard_integration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Create a README file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "def create_readme():\n",
    "    \"\"\"\n",
    "    Create a README file for the project.\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating README file...\")\n",
    "    \n",
    "    with open('README.md', 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "# Energy Consumption Prediction System\n",
    "\n",
    "A comprehensive system for predicting and managing household energy consumption using machine learning and time series forecasting techniques.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project implements a complete end-to-end solution for energy consumption prediction, from data collection and preprocessing to model training, evaluation, and deployment. The system helps households manage and reduce their energy usage by providing accurate forecasts and actionable insights.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Data Collection**: Simulated smart meter and weather data (extensible to real API integrations)\n",
    "- **Data Preprocessing**: Handling missing values, outliers, and feature engineering\n",
    "- **Model Training**: Multiple models implemented (Gradient Boosting, XGBoost, LightGBM, LSTM, Prophet)\n",
    "- **Model Evaluation**: Comprehensive performance metrics and visualizations\n",
    "- **Deployment**: Flask API for real-time predictions\n",
    "- **Energy Savings Calculator**: Tools to estimate potential savings from different strategies\n",
    "- **Dashboard Integration**: Example code for integration with visualization dashboards\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "├── app.py                     # Flask API for model deployment\n",
    "├── Dockerfile                 # Docker configuration for deployment\n",
    "├── requirements.txt           # Python dependencies\n",
    "├── README.md                  # Project documentation\n",
    "├── data/                      # Data storage\n",
    "│   ├── raw_energy_data.csv    # Raw energy consumption data\n",
    "│   ├── raw_weather_data.csv   # Raw weather data\n",
    "│   └── processed_data.csv     # Processed and merged dataset\n",
    "├── models/                    # Trained models and pipelines\n",
    "│   ├── gradient_boosting_model.pkl\n",
    "│   ├── xgboost_model.pkl\n",
    "│   ├── lightgbm_model.pkl\n",
    "│   ├── lstm_model.h5\n",
    "│   ├── prophet_model.pkl\n",
    "│   ├── scaler.pkl             # Feature scaler\n",
    "│   ├── lstm_params.json       # LSTM parameters\n",
    "│   └── prediction_pipeline.py # Prediction pipeline class\n",
    "└── results/                   # Visualizations and analysis results\n",
    "    ├── energy_consumption_time_series.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ├── hourly_energy_pattern.png\n",
    "    ├── weekly_energy_pattern.png\n",
    "    ├── monthly_energy_pattern.png\n",
    "    ├── energy_vs_temperature.png\n",
    "    ├── correlation_matrix.png\n",
    "    ├── weekend_vs_weekday.png\n",
    "    ├── gb_predictions.png\n",
    "    ├── xgb_predictions.png\n",
    "    ├── lgb_predictions.png\n",
    "    ├── lstm_predictions.png\n",
    "    ├── prophet_predictions.png\n",
    "    ├── model_comparison.png\n",
    "    ├── best_model_test_predictions.png\n",
    "    ├── best_model_feature_importance.png\n",
    "    ├── scenario_comparison.png\n",
    "    ├── savings_comparison.png\n",
    "    ├── dashboard_preview.png\n",
    "    └── dashboard_data.json\n",
    "```\n",
    "\n",
    "## Installation and Setup\n",
    "\n",
    "1. Clone the repository:\n",
    "```bash\n",
    "git clone https://github.com/yourusername/energy-consumption-prediction.git\n",
    "cd energy-consumption-prediction\n",
    "```\n",
    "\n",
    "2. Install dependencies:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "3. Run the Jupyter notebook to train models and generate results:\n",
    "```bash\n",
    "jupyter notebook energy_consumption_prediction.ipynb\n",
    "```\n",
    "\n",
    "4. Start the Flask API:\n",
    "```bash\n",
    "python app.py\n",
    "```\n",
    "\n",
    "5. Or build and run with Docker:\n",
    "```bash\n",
    "docker build -t energy-prediction .\n",
    "docker run -p 5000:5000 energy-prediction\n",
    "```\n",
    "\n",
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Making Predictions with the API\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Single prediction\n",
    "data = {\n",
    "    'timestamp': '2023-04-10T12:00:00Z',\n",
    "    'weather': {\n",
    "        'temperature': 22.5,\n",
    "        'humidity': 65.0,\n",
    "        'wind_speed': 8.2,\n",
    "        'cloud_cover': 40.0,\n",
    "        'precipitation': 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post('http://localhost:5000/predict', json=data)\n",
    "predictions = response.json()\n",
    "print(predictions)\n",
    "\n",
    "# Batch prediction\n",
    "files = {'file': open('path/to/your/input_data.csv', 'rb')}\n",
    "response = requests.post('http://localhost:5000/batch_predict', files=files)\n",
    "batch_predictions = response.json()\n",
    "print(batch_predictions)\n",
    "```\n",
    "\n",
    "### Using the Energy Savings Calculator\n",
    "\n",
    "```python\n",
    "from energy_savings_calculator import EnergySavingsCalculator\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('your_energy_data.csv')\n",
    "\n",
    "# Initialize calculator\n",
    "calculator = EnergySavingsCalculator()\n",
    "\n",
    "# Compare different scenarios\n",
    "results = calculator.compare_scenarios(data)\n",
    "\n",
    "# Plot comparison\n",
    "calculator.plot_comparison(results)\n",
    "```\n",
    "\n",
    "### Dashboard Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "```python\n",
    "from dashboard_integration import generate_dashboard\n",
    "\n",
    "# Generate dashboard data and preview\n",
    "dashboard_data = generate_dashboard()\n",
    "print(dashboard_data['current_stats'])\n",
    "```\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "The system trains and evaluates multiple models:\n",
    "\n",
    "1. Gradient Boosting Regressor\n",
    "2. XGBoost\n",
    "3. LightGBM\n",
    "4. LSTM Neural Network\n",
    "5. Prophet\n",
    "\n",
    "Performance metrics including RMSE, MAE, R², and MAPE are calculated for each model, and the best-performing model is selected for deployment.\n",
    "\n",
    "## Energy Saving Strategies\n",
    "\n",
    "The system evaluates the impact of various energy-saving strategies:\n",
    "\n",
    "1. **Temperature Reduction**: Adjusting temperature settings during summer months\n",
    "2. **Smart Scheduling**: Shifting energy usage from peak to off-peak hours\n",
    "3. **Efficient Appliances**: Reducing overall consumption through more efficient devices\n",
    "\n",
    "## License\n",
    "\n",
    "This project is licensed under the MIT License - see the LICENSE file for details.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- Data generation based on typical household consumption patterns\n",
    "- Weather patterns based on typical seasonal variations\n",
    "- Inspired by real-world energy management systems\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"README file created.\")\n",
    "\n",
    "# Create README\n",
    "create_readme()\n",
    "\n",
    "# Run the main analysis\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\nEnergy Consumption Prediction System Created Successfully!\")\n",
    "print(\"\\nProject Structure:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- data/: Contains raw and processed data\")\n",
    "print(\"- models/: Contains trained models and prediction pipeline\")\n",
    "print(\"- results/: Contains visualizations and analysis results\")\n",
    "print(\"- app.py: Flask API for model deployment\")\n",
    "print(\"- energy_savings_calculator.py: Tool for estimating energy savings\")\n",
    "print(\"- dashboard_integration.py: Example code for dashboard integration\")\n",
    "print(\"- README.md: Project documentation\")\n",
    "print(\"- requirements.txt: Python dependencies\")\n",
    "print(\"- Dockerfile: Docker configuration for deployment\")\n",
    "\n",
    "print(\"\\nThis notebook implements a comprehensive energy consumption prediction system\")\n",
    "print(\"that can be used for household energy management and savings.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
